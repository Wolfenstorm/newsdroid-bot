import asyncio
import httpx
import feedparser
import json
import os
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è ---
BOT_TOKEN = "7577336852:AAGz1yyJen2HRGVzwLQB3HVWa75fO-bZ9pU"
CHAT_ID = "-1002650930903"

RSS_FEEDS = [
    "https://rss.unian.net/site/news_ukr.rss",
    "https://www.epravda.com.ua/rss/main/",
    "https://www.eurointegration.com.ua/rss/",
    "https://www.radiosvoboda.org/api/zrqiteuu$ute",
    "https://novynarnia.com/feed/",
    "https://gordonua.com/xml/rss.xml",
    "https://lb.ua/export/rss/all.xml",
    "https://www.unn.com.ua/uk/rss/news_ukr.xml",
    "https://www.ukrinform.ua/rss/block-lastnews",
    "https://www.ukrinform.ua/rss/block-world",
    "https://zn.ua/rss.xml",
    "https://suspilne.media/rss/news.xml",
    "https://www.5.ua/rss",
    "https://hromadske.ua/rss",
    "https://glavcom.ua/rss/all.xml",
    "https://apostrophe.ua/rss",
    "https://interfax.com.ua/news/general.rss",
    "https://news.liga.net/politics/rss.xml",
    "https://mind.ua/rss/news",
    "https://life.pravda.com.ua/rss/",
    "https://armyinform.com.ua/feed/"
]

SOURCE_PRIORITY = {
    "unian.net": 1,
    "radiosvoboda.org": 2
}

POSTED_FILE = "posted.json"

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∂–µ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏—Ö –Ω–æ–≤–∏–Ω ---
def load_posted_urls():
    if os.path.exists(POSTED_FILE):
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

# --- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–∏—Ö –Ω–æ–≤–∏–Ω ---
def save_posted_urls(posted_urls):
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(list(posted_urls), f, ensure_ascii=False, indent=2)

# --- –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–æ–º–µ–Ω—É (–¥–ª—è –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É) ---
def get_domain(url):
    parsed = urlparse(url)
    return parsed.netloc.replace("www.", "")

# --- –û—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–æ–≤–∏–Ω –∑–∞ –æ—Å—Ç–∞–Ω–Ω—é –≥–æ–¥–∏–Ω—É ---
def fetch_recent_news():
    one_hour_ago = datetime.now(timezone.utc) - timedelta(hours=1)
    news_items = []

    for feed_url in RSS_FEEDS:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            if hasattr(entry, 'published_parsed'):
                pub_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                if pub_time >= one_hour_ago:
                    domain = get_domain(entry.link)
                    source_score = SOURCE_PRIORITY.get(domain, 99)
                    news_items.append({
                        "title": entry.title,
                        "url": entry.link,
                        "source_score": source_score,
                        "length_score": len(entry.title),
                        "published": pub_time.isoformat()
                    })
    return news_items

# --- –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è ---
async def publish_post(chat_id, message):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": message,
        "disable_web_page_preview": False,
        "parse_mode": "HTML"
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(url, json=payload)
            print(f"[‚úÖ] Telegram: {response.status_code} - {response.text}")
            response.raise_for_status()
            return True
        except httpx.HTTPStatusError as e:
            print(f"[!] HTTP error: {e.response.status_code} - {e.response.text}")
            return False
        except Exception as e:
            print(f"[!] Unexpected error: {e}")
            return False

# --- –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –Ω–æ–≤–∏–Ω ---
async def main():
    print("üü¢ –°—Ç–∞—Ä—Ç. –®—É–∫–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—é –≥–æ–¥–∏–Ω—É...")

    posted_urls = load_posted_urls()
    recent_news = fetch_recent_news()

    new_news = [
        news for news in recent_news
        if news["url"] not in posted_urls
    ]

    print(f"üì• –ù–æ–≤–∏–Ω –∑–∞ –æ—Å—Ç–∞–Ω–Ω—é –≥–æ–¥–∏–Ω—É: {len(recent_news)} | –ù–æ–≤–∏—Ö: {len(new_news)}")

    sorted_news = sorted(
        new_news,
        key=lambda n: (n["source_score"], -n["length_score"], n["published"])
    )[:21]

    for news in sorted_news:
        message = f"<b>{news['title']}</b>\n{news['url']}"
        success = await publish_post(CHAT_ID, message)
        if success:
            posted_urls.add(news["url"])
        await asyncio.sleep(2)

    save_posted_urls(posted_urls)
    print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–æ–≤–∏–Ω–∏ –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω—ñ.")

# --- –ë–µ–∑–∫—ñ–Ω–µ—á–Ω–∏–π —Ü–∏–∫–ª (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –∑–∞–ø—É—Å–∫ —â–æ–≥–æ–¥–∏–Ω–∏) ---
async def loop_forever():
    while True:
        await main()
        print("‚è≥ –û—á—ñ–∫—É—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω—É –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —á–µ—Ä–µ–∑ 1 –≥–æ–¥–∏–Ω—É...")
        await asyncio.sleep(3600)

if __name__ == "__main__":
    try:
        asyncio.run(loop_forever())
    except KeyboardInterrupt:
        print("üõë –ó—É–ø–∏–Ω–µ–Ω–æ –≤—Ä—É—á–Ω—É.")
